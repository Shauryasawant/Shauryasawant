from google.colab import files
import pandas as pd
from playwright.async_api import async_playwright
from urllib.parse import urljoin, urlparse
import asyncio
import nest_asyncio
import logging
from datetime import datetime, timedelta
import re
import requests
from geopy.distance import geodesic
import time
from rapidfuzz import fuzz
from concurrent.futures import ThreadPoolExecutor, as_completed

# Enable nested event loops for Colab
nest_asyncio.apply()

# [Previous WebsiteChecker class code goes here - ]
class WebsiteChecker:
    def __init__(self):
        self.logger = logging.getLogger(__name__)
        logging.basicConfig(level=logging.INFO)

        # Add POI-specific patterns
        self.poi_activity_patterns = {
            'opening_hours': r'(?:opening|business|hours|öffnungszeiten).*?(?:\d{1,2}:\d|closed|open)',
            'booking': r'(?:book|reserve|appointment|termin|reservierung)',
            'recent_post': r'(?:posted|updated|aktualisiert).*?(?:today|yesterday|heute|gestern|\d{1,2}\s+(?:day|tag))',
            'social_media': r'(?:facebook\.com|instagram\.com|twitter\.com)\/\w+',
            'contact_form': r'(?:contact|kontakt|message|nachricht).{0,20}(?:form|formular)',
            'recent_prices': r'(?:€|EUR|euro).{0,10}\d+[.,]\d',
            'menu': r'(?:speisekarte|menu|carte).{0,30}(?:pdf|\d)',
        }

        # Add closure patterns
        self.closure_patterns = {
            'permanently_closed': [
                r'permanently\s*closed',
                r'geschlossen\s*(?:für\s*immer|dauerhaft)',
                r'closed\s*down',
                r'(?:business|store)\s*(?:no\s*longer\s*operating|defunct)',
                r'nicht\s*mehr\s*existierend'
            ],
            'temporarily_closed': [
                r'temporarily\s*closed',
                r'vorübergehend\s*geschlossen',
                r'closed\s*for\s*(?:renovation|umbau)',
                r'currently\s*not\s*available',
                r'aktuell\s*geschlossen'
            ]
        }

    def normalize_text(self, text):
        if pd.isna(text):
            return ''
        text = text.lower()
        replacements = {
            'strasse': 'str', 'straße': 'str', 'str.': 'str',
            'chaussee': 'ch', 'platz': 'pl', 'allee': 'al',
            'ä': 'ae', 'ö': 'oe', 'ü': 'ue', 'ß': 'ss'
        }
        for old, new in replacements.items():
            text = text.replace(old, new)
        text = re.sub(r'[^\w\s0-9-]', '', text)
        return ' '.join(text.split())

    def check_closure_status(self, text):
        """Check for closure indicators."""
        normalized_text = self.normalize_text(text)

        for closure_type, patterns in self.closure_patterns.items():
            for pattern in patterns:
                if re.search(pattern, normalized_text, re.IGNORECASE):
                    return closure_type
        return None

    def split_address(self, address):
        """Enhanced split_address with better component handling."""
        if pd.isna(address):
            return []

        # Split by common delimiters while preserving hyphenated parts
        parts = []
        temp_parts = re.split(r'[,\s]+', str(address))

        for part in temp_parts:
            # Handle hyphenated parts
            if '-' in part:
                # Add both the full hyphenated part and individual components
                parts.append(part)
                parts.extend(part.split('-'))
            else:
                parts.append(part)

        # Clean and filter parts
        cleaned_parts = []
        for part in parts:
            # Remove common address prefixes/suffixes
            part = re.sub(r'^(DE-|Nr\.|no\.|number)', '', part, flags=re.IGNORECASE)
            # Normalize and clean
            part = self.normalize_text(part)
            # Only keep non-empty parts and allow shorter street numbers
            if part and (len(part) > 1 or part.isdigit()):
                cleaned_parts.append(part)

        return list(set(cleaned_parts))  # Remove duplicates

    def extract_url_components(self, url):
        """Extract potential address components from URL."""
        try:
            # Parse URL path
            parsed = urlparse(url)
            path_components = parsed.path.split('/')

            # Filter out empty components and common URL parts
            components = [comp for comp in path_components if comp and comp not in
                         {'tankstelle', 'gas', 'station', 'de', 'en', 'stations'}]

            # Clean components
            cleaned_components = []
            for comp in components:
                # Replace hyphens with spaces and clean
                comp = comp.replace('-', ' ')
                comp = self.normalize_text(comp)
                if comp:
                    cleaned_components.append(comp)

            return cleaned_components
        except Exception:
            return []

    def check_address_match(self, text, row_data, url=''):
        """Enhanced address matching with tracking for all columns."""
        normalized_page_text = self.normalize_text(text)
        found_terms = []
        matches_by_column = {}

        # Extract potential address components from URL
        url_components = self.extract_url_components(url) if url else []

        # Add URL components to the search text
        if url_components:
            normalized_page_text += ' ' + ' '.join(url_components)

        # Define all columns to check
        columns_to_check = [
            'unparsed_addr', 'admin1', 'admin2', 'admin3', 'admin4', 'admin5',
            'postal_code', 'street', 'house_number', 'city', 'state', 'country'
        ]

        for column in columns_to_check:
            term = row_data.get(column, '')
            if pd.isna(term):
                continue

            # Split each address term into components
            address_parts = self.split_address(str(term))
            matched_parts = []

            for part in address_parts:
                if part in normalized_page_text:
                    matched_parts.append(part)
                elif len(part) > 3:  # Try partial matching for longer terms
                    if any(part in comp or comp in part for comp in normalized_page_text.split()):
                        matched_parts.append(part)

            # Define minimum required matches based on total parts
            min_required = 1 if len(address_parts) <= 2 else 2

            # Record matches if found
            if len(matched_parts) >= min_required:
                matches_by_column[column] = term
                found_terms.append(term)
                self.logger.info(f"Found matches for : ")

        return found_terms, matches_by_column

    def check_poi_activity(self, content):
        """Check for indicators of POI activity on the website."""
        poi_indicators = {
            'activity_indicators': [],
            'activity_score': 0
        }

        # Check each pattern for POI activity
        for indicator, pattern in self.poi_activity_patterns.items():
            matches = re.finditer(pattern, content, re.IGNORECASE)
            found_matches = list(matches)

            if found_matches:
                poi_indicators['activity_indicators'].append(indicator)
                # Assign different weights to different indicators
                if indicator in ['opening_hours', 'recent_post']:
                    poi_indicators['activity_score'] += 3
                elif indicator in ['booking', 'contact_form', 'recent_prices']:
                    poi_indicators['activity_score'] += 2
                else:
                    poi_indicators['activity_score'] += 1

        return poi_indicators

    def get_url_variations(self, url):
        """Generate URL variations and skip irrelevant pages."""
        parsed = urlparse(url)
        base_domain = parsed.netloc.replace('www.', '')

        variations = [
            url.rstrip('/'),
            f"https://{base_domain}",
            f"http://{base_domain}",
            f"https://www.{base_domain}",
            f"http://www.{base_domain}"
        ]

        # Avoid pages with irrelevant paths
        avoid_paths = ['menu', 'gallery', 'careers', 'blog', 'news']

        return [v for v in set(variations) if not any(skip in v for skip in avoid_paths)]

    async def get_page_content(self, page):
        """Get text content from main page and iframes."""
        main_text = await page.inner_text('body')

        iframe_text = ''
        for frame in page.frames:
            try:
                if frame != page.main_frame:
                    iframe_text += await frame.inner_text('body') + ' '
            except Exception as e:
                self.logger.debug(f"Error getting iframe content: {str(e)}")
                continue

        return f"{main_text} {iframe_text}"

    def extract_date_and_copyright_info(self, text):
        """Extract recent reviews and copyright information."""
        current_year = datetime.now().year
        last_year = current_year - 1

        copyright_pattern = r'(?:\u00a9|\(c\)|copyright)\s*(\d{4})'
        recent_review_threshold = datetime.now() - timedelta(days=182)

        found_info = {
            'recent_review': False,
            'copyright_year': None
        }

        # Check for copyright years
        for match in re.finditer(copyright_pattern, text, flags=re.IGNORECASE):
             year = int(match.group(1))
             if last_year <= year <= current_year:
                 found_info['copyright_year'] = f"Copyright {year}"
                 break
        # Check for recent reviews
        date_patterns = [r'\d{1,2}[-./]\d{1,2}[-./]\d{2,4}']
        for pattern in date_patterns:
            for date_match in re.findall(pattern, text):
                try:
                    # Adjust date parsing based on the match
                    if '-' in date_match:
                        date = datetime.strptime(date_match, '%d-%m-%Y')
                    elif '/' in date_match:
                         date = datetime.strptime(date_match, '%d/%m/%Y')
                    else:
                        date = datetime.strptime(date_match, '%d.%m.%Y')

                    if date >= recent_review_threshold:
                       found_info['recent_review'] = True
                       break
                except ValueError:
                    continue

        return found_info

    async def check_website(self, url, row_data):
        """Check website with enhanced address matching."""
        url_variations = self.get_url_variations(url)
        self.logger.info(f"Checking {len(url_variations)} URL variations for ")

        for variant_url in url_variations:
            try:
                async with async_playwright() as p:
                    browser = await p.chromium.launch(headless=True)
                    context = await browser.new_context(
                        user_agent='Mozilla/5.0 (Windows NT 10.0; Win64; x64) Chrome/91.0.4472.124'
                    )
                    page = await context.new_page()
                    page.set_default_timeout(60000)
                    await page.goto(variant_url, wait_until='networkidle')

                    await page.evaluate("window.scrollTo(0, document.body.scrollHeight)")
                    await page.wait_for_timeout(2000)

                    content = await self.get_page_content(page)
                    found_terms, matches_by_column = self.check_address_match(content, row_data, url)
                    date_info = self.extract_date_and_copyright_info(content)
                    poi_info = self.check_poi_activity(content)
                    closure_status = self.check_closure_status(content)

                    await browser.close()

                     # Determine is_active with closure status
                    is_active = (not closure_status and
                                 (found_terms or date_info['recent_review'] or
                                  date_info['copyright_year'] or
                                  poi_info['activity_score'] >= 3))

                    return {
                        'is_active': is_active,
                        'found_terms': found_terms,
                        'matches_by_column': matches_by_column,
                        'recent_review': date_info['recent_review'],
                        'copyright_year': date_info['copyright_year'],
                        'found_url': variant_url,
                        'poi_activity': poi_info,
                        'closure_status': closure_status,
                        'error': None
                    }
            except Exception as e:
                self.logger.debug(f"Error checking : {str(e)}")
                continue

        return {
            'is_active': False,
            'found_terms': [],
            'matches_by_column': {},
            'recent_review': False,
            'copyright_year': None,
            'found_url': None,
             'poi_activity': {'activity_indicators': [], 'activity_score': 0},
            'closure_status': None,
            'error': "No active content found on any URL variation"
        }

    def get_status_reason(self, result):
        """Generate status reason with detailed column matches and closure status."""
        closure_status = result.get('closure_status')

        if result['error']:
            return f"Error: {result['error']}"

        # Combine existing logic with closure status
        status_parts = []

        if closure_status:
             status_parts.append(f"Store Status: {closure_status}")

        # Existing status logic
        if result['matches_by_column'] and not result['recent_review'] and not result['poi_activity']['activity_indicators']:
            address_match = "Matches found in: " + ", ".join(result['matches_by_column'].keys())
            if result['copyright_year']:
                address_match += f" | {result['copyright_year']}"
            status_parts.append(address_match)
        elif result['is_active']:
            if result['matches_by_column']:
                match_details = [f"{col}: {term}" for col, term in result['matches_by_column'].items()]
                status_parts.append(f"Matches found in [{', '.join(match_details)}]")

            if result['recent_review']:
                status_parts.append("Recent reviews found")
            if result['copyright_year']:
                status_parts.append(f"Copyright found: {result['copyright_year']}")

            activities = []
            if result.get('poi_activity', {}).get('activity_indicators'):
                for indicator in result['poi_activity']['activity_indicators']:
                    if indicator == 'opening_hours':
                        activities.append('hours')
                    elif indicator == 'recent_post':
                        activities.append('recent updates')
                    else:
                        activities.append(indicator)

            if activities:
                status_parts.append(f"Found: {', '.join(activities)}")

            if result['found_url']:
                status_parts.append(f"URL: {result['found_url']}")

        return " | ".join(status_parts) if status_parts else "No activity indicators found"
# [Previous validation functions code goes here - ]
def get_nearby_places(query, lat, lng, api_key, radius=1000):
    """Search for places near a specific location using Places API."""
    url = "https://maps.gomaps.pro/maps/api/place/nearbysearch/json"

    params = {
        "location": f"{lat},{lng}",
        "radius": radius,
        "keyword": query,
        "key": api_key
    }

    try:
        response = requests.get(url, params=params)
        data = response.json()

        if data.get('status') == 'OK':
            return data.get('results', [])
        else:
            print(f"API Error: {data.get('status')} - {data.get('error_message', 'No error message')}")
            return []

    except Exception as e:
        print(f"Error in get_nearby_places: {str(e)}")
        return []

def get_place_details(place_id, api_key):
    """Get detailed place information including categories and phone number."""
    url = "https://maps.gomaps.pro/maps/api/place/details/json"
    query_params = {
        "place_id": place_id,
        "key": api_key,
        "fields": "address_components,formatted_address,opening_hours,reviews,business_status,types,url,website,formatted_phone_number",
        "reviews_sort": "newest"
    }

    try:
        response = requests.get(url, params=query_params)
        data = response.json()

        if data.get('status') == 'OK':
            result = data.get('result', {})
            address_components = result.get('address_components', [])

            state = None
            county = None
            city = None
            postal_code = None
            street_number = None
            street_name = None
            admin3 = None
            admin5 = None

            for component in address_components:
                types = component.get('types', [])
                if 'administrative_area_level_1' in types:
                    state = component.get('short_name')
                elif 'administrative_area_level_2' in types:
                    county = component.get('long_name')
                elif 'locality' in types:
                    city = component.get('long_name')
                elif 'postal_code' in types:
                    postal_code = component.get('long_name')
                elif 'street_number' in types:
                    street_number = component.get('long_name')
                elif 'route' in types:
                    street_name = component.get('long_name')
                elif 'administrative_area_level_3' in types:
                    admin3 = component.get('long_name')
                elif 'sublocality_level_1' in types or 'sublocality' in types:
                    admin5 = component.get('long_name')

            maps_url = f"https://www.google.com/maps/place/?q=place_id:{place_id}"

            if 'reviews' in result:
                result['reviews'] = sorted(
                    result['reviews'],
                    key=lambda x: x.get('time', 0),
                    reverse=True
                )

            return {
                'state': state,
                'county': county,
                'city': city,
                'postal_code': postal_code,
                'street_number': street_number,
                'street_name': street_name,
                'admin3': admin3,
                'admin5': admin5,
                'business_status': result.get('business_status'),
                'types': result.get('types', []),
                'reviews': result.get('reviews', []),
                'maps_url': maps_url,
                'website': result.get('website', ''),
                'formatted_address': result.get('formatted_address'),
                'phone_number': result.get('formatted_phone_number', '')
            }

    except Exception as e:
        print(f"Error getting place details: {str(e)}")
    return None

def normalize_category(category):
    """Normalize category names and handle special cases."""
    category = category.lower().strip()

    category_mappings = {
        'other': ['pharmacy', 'convenience_store', 'store', 'health'],
        'farmers and markets': ['grocery_or_supermarket', 'supermarket', 'store', 'food'],
        'super stores': ['supermarket', 'grocery_or_supermarket', 'department_store', 'store']
    }

    for key, values in category_mappings.items():
        if category in key:
            return values
    return [category]

def is_place_active(place_details):
    """Check if place is active based on business status and recent reviews."""
    if not place_details:
        return "UNKNOWN"

    business_status = place_details.get('business_status', '').upper()

    if business_status == 'OPERATIONAL':
        reviews = place_details.get('reviews', [])
        if reviews:
            current_time = datetime.now()
            six_months_ago = current_time - timedelta(days=180)

            # First, explicitly sort reviews by timestamp (newest first)
            sorted_reviews = sorted(reviews, key=lambda x: x.get('time', 0), reverse=True)

            # Check ALL reviews from newest to oldest until we find one with text
            for review in sorted_reviews:
                review_time = datetime.fromtimestamp(review.get('time', 0))
                review_text = review.get('text', '').strip()

                # If we find a review with text within 6 months, return YES
                if review_time >= six_months_ago and review_text:
                    return "YES"
                # If we find a review older than 6 months, we can stop checking
                elif review_time < six_months_ago:
                    break

        return "UNKNOWN"
    elif 'TEMPORARILY_CLOSED' in business_status or 'CLOSED_TEMPORARILY' in business_status:
        return "TEMPORARILY_CLOSED"
    elif 'PERMANENTLY_CLOSED' in business_status or 'CLOSED_PERMANENTLY' in business_status:
        return "PERMANENTLY_CLOSED"
    else:
        return "UNKNOWN"

def is_plus_code(text):
    """Check if the text matches Plus Code format."""
    # Plus Code formats: "XXXX+XX" or "XXXX+XX City Name"
    plus_code_pattern = r'^[23456789CFGHJMPQRVWX]\+[23456789CFGHJMPQRVWX]{2,4}(?:\s.*)?$'
    if not text:
        return False
    return bool(re.match(plus_code_pattern, str(text).strip()))

def compare_addresses(addr1, addr2):
    if not addr1 or not addr2:
        return "NO"
    addr1_clean = ' '.join(str(addr1).upper().strip().split())
    addr2_clean = ' '.join(str(addr2).upper().strip().split())

    # Check if either address is a Plus Code
    if is_plus_code(addr1_clean) or is_plus_code(addr2_clean):
        return "YES"

    if addr1_clean == addr2_clean or addr1_clean in addr2_clean or addr2_clean in addr1_clean:
        return "YES"
    if fuzz.token_set_ratio(addr1_clean, addr2_clean) >= 80:
        return "YES"
    return "NO"

def compare_phone_numbers(sheet_phone, google_phone):
    """Compare phone numbers by removing all non-numeric characters and handling international format."""
    if not sheet_phone or not google_phone:
        return "NO"

    def clean_phone(phone):
        # Remove all non-numeric characters
        cleaned = ''.join(filter(str.isdigit, str(phone)))

        # If it starts with international prefix '1' for US numbers, remove it
        if len(cleaned) > 10 and cleaned.startswith('1'):
            cleaned = cleaned[1:]

        # Get last 10 digits if number is longer
        if len(cleaned) >= 10:
            cleaned = cleaned[-10:]

        return cleaned

    sheet_clean = clean_phone(sheet_phone)
    google_clean = clean_phone(google_phone)

    return "YES" if sheet_clean == google_clean else "NO"

def preprocess_name(name):
    """Comprehensive name preprocessing with enhanced handling of special cases."""
    if not name:
        return ""

    # Convert to lowercase and strip
    name = name.lower().strip()

    # Remove 'name:' prefix if present
    name = name.replace("name:", "").strip()

    # Expanded business terms and special handling
    business_terms = {
        # Business identifiers and descriptive words to remove
        'gmbh', 'ltd', 'limited', 'llc', 'inc', 'incorporated',
        'corp', 'corporation', 'co', 'company', 'enterprise',
        'enterprises', 'group', 'holdings', 'partners', 'services',
        'solutions', 'technologies',

        # Retail and location specific terms to potentially remove
        'store', 'shop', 'center', 'centre', 'mart', 'market',
        'supermarket', 'optical', 'vision', 'automotive', 'auto',
        'repair', 'beach', 'hotel', 'eye', 'sunglass',

        # Conjunctions and prepositions to remove
        'the', 'and', 'of', 'at', 'in', 'on', 'by', 'to',
        'for', 'with', 'a', 'an', 'or', '&'
    }

    # Enhanced character mapping
    char_map = {
        '@': 'at', '&': 'and', '+': 'plus',
        '-': ' ', '_': ' ', '/': ' ', '\\': ' ',
        '.': ' ', ',': ' ', ':': ' ', ';': ' ',
        '|': ' ', '(': ' ', ')': ' ', '[': ' ', ']': ' ',
        '{': ' ', '}': ' ', '*': ' ', '#': ' ', '%': ' '
    }

    # Apply character mapping
    for char, replacement in char_map.items():
        name = name.replace(char, replacement)

    # Split into words and process
    words = name.split()
    processed_words = []

    for word in words:
        # Keep the core name, remove descriptive terms
        if word not in business_terms or len(word) > 4:
            processed_words.append(word)

    # Join processed words, prioritizing meaningful content
    processed_name = ' '.join(processed_words)

    return processed_name.strip()

def check_name_similarity(original_name, google_name):
    """Enhanced name matching with multiple similarity strategies."""
    if not original_name or not google_name:
        return "NO"

    # Preprocess both names
    orig_processed = preprocess_name(original_name)
    google_processed = preprocess_name(google_name)

    # Direct match after preprocessing
    if orig_processed == google_processed:
        return "YES"

    # Use fuzzy matching with multiple strategies
    from rapidfuzz import fuzz

    # Check if original name is a subset of Google name
    if orig_processed and orig_processed in google_processed:
        return "YES"

    # Calculate various similarity metrics
    token_set_ratio = fuzz.token_set_ratio(orig_processed, google_processed)
    token_sort_ratio = fuzz.token_sort_ratio(orig_processed, google_processed)
    partial_ratio = fuzz.partial_ratio(orig_processed, google_processed)

    # Split into words for detailed comparison
    orig_words = set(orig_processed.split())
    google_words = set(google_processed.split())

    # Word overlap calculation
    word_overlap = len(orig_words.intersection(google_words))
    max_words = max(len(orig_words), len(google_words))
    overlap_ratio = word_overlap / max_words if max_words > 0 else 0

    # Distinctive word matching (words longer than 3 characters)
    distinctive_orig = {w for w in orig_words if len(w) > 3}
    distinctive_google = {w for w in google_words if len(w) > 3}

    # Distinctive word overlap
    if distinctive_orig and distinctive_google:
        distinctive_overlap = len(distinctive_orig.intersection(distinctive_google))
        distinctive_ratio = distinctive_overlap / len(distinctive_orig)
    else:
        distinctive_ratio = 0

    # Flexible thresholds based on name characteristics
    if max_words <= 2:
        # Stricter thresholds for very short names
        thresholds = {
            'set_ratio': 85,
            'sort_ratio': 85,
            'partial_ratio': 90,
            'overlap_ratio': 0.7,
            'distinctive_ratio': 0.7
        }
    else:
        # More lenient thresholds for longer names
        thresholds = {
            'set_ratio': 75,
            'sort_ratio': 75,
            'partial_ratio': 80,
            'overlap_ratio': 0.6,
            'distinctive_ratio': 0.6
        }

    # Comprehensive matching criteria
    matching_criteria = [
        token_set_ratio >= thresholds['set_ratio'],
        token_sort_ratio >= thresholds['sort_ratio'],
        partial_ratio >= thresholds['partial_ratio'],
        overlap_ratio >= thresholds['overlap_ratio'],
        distinctive_ratio >= thresholds['distinctive_ratio']
    ]

    # If any matching criterion is met, consider it a match
    return "YES" if any(matching_criteria) else "NO"

def check_location_accuracy(distance):
    """Check if the location is within acceptable range."""
    if distance <= 10:
        return "OPTIMAL"
    elif distance <= 15:
        return "ACCEPTABLE"
    else:
        return "INCORRECT"

def check_category_match(original_category, google_types):
    """Enhanced category matching with special categories support."""
    if not original_category or not google_types:
        return "NO"

    normalized_expected = normalize_category(original_category)
    normalized_types = [t.lower().replace('_', ' ').strip() for t in google_types]

    for expected in normalized_expected:
        expected = expected.replace('_', ' ')

        if expected in normalized_types:
            return "YES"

        for google_type in normalized_types:
            if expected in google_type or google_type in expected:
                return "YES"

            if fuzz.ratio(expected, google_type) >= 80:
                return "YES"

    return "NO"

def validate_street_name(sheet_street, google_street):
    if not sheet_street or not google_street:
        return "NO"
    sheet_street = str(sheet_street).lower().strip()
    google_street = str(google_street).lower().strip()

    # Check for Plus Code in street name
    if is_plus_code(sheet_street) or is_plus_code(google_street):
        return "YES"

    replacements = {'street': 'st', 'avenue': 'ave', 'boulevard': 'blvd', 'road': 'rd', 'drive': 'dr', 'lane': 'ln', 'circle': 'cir', 'court': 'ct', 'place': 'pl', 'highway': 'hwy'}
    for full, abbr in replacements.items():
        sheet_street = sheet_street.replace(full, abbr)
        google_street = google_street.replace(full, abbr)
    return "YES" if fuzz.ratio(sheet_street, google_street) >= 85 else "NO"

def validate_admin2(sheet_admin2, google_state):
    if not sheet_admin2 or not google_state:
        return "NO"
    sheet_admin2 = str(sheet_admin2).strip().upper()
    google_state = str(google_state).strip().upper()

    # Check for Plus Code
    if is_plus_code(sheet_admin2) or is_plus_code(google_state):
        return "YES"

    return "YES" if sheet_admin2 == google_state else "NO"

def validate_admin3(sheet_admin3, google_admin3):
    if not sheet_admin3 or not google_admin3:
        return "NO"
    sheet_admin3 = str(sheet_admin3).lower().strip()
    google_admin3 = str(google_admin3).lower().strip()

    # Check for Plus Code
    if is_plus_code(sheet_admin3) or is_plus_code(google_admin3):
        return "YES"

    return "YES" if fuzz.ratio(sheet_admin3, google_admin3) >= 85 else "NO"

def validate_admin4(sheet_admin4, google_city):
    if not sheet_admin4 or not google_city:
        return "NO"
    sheet_admin4 = str(sheet_admin4).lower().strip()
    google_city = str(google_city).lower().strip()

    # Check for Plus Code
    if is_plus_code(sheet_admin4) or is_plus_code(google_city):
        return "YES"

    # More lenient matching for admin4
    if sheet_admin4 == google_city:
        return "YES"
    if fuzz.ratio(sheet_admin4, google_city) >= 85:
        return "YES"
    if fuzz.token_set_ratio(sheet_admin4, google_city) >= 85:
        return "YES"
    return "NO"

def validate_admin5(sheet_admin5, google_admin5):
    if not sheet_admin5 or not google_admin5:
        return "NO"
    sheet_admin5 = str(sheet_admin5).lower().strip()
    google_admin5 = str(google_admin5).lower().strip()

    # Check for Plus Code
    if is_plus_code(sheet_admin5) or is_plus_code(google_admin5):
        return "YES"

    # More lenient matching for admin5
    if sheet_admin5 == google_admin5:
        return "YES"
    if fuzz.ratio(sheet_admin5, google_admin5) >= 85:
        return "YES"
    if fuzz.token_set_ratio(sheet_admin5, google_admin5) >= 85:
        return "YES"
    return "NO"

def validate_postal_code(sheet_postal, google_postal):
    if not sheet_postal or not google_postal:
        return "NO"
    sheet_postal = str(sheet_postal).strip().split('-')[0]
    google_postal = str(google_postal).strip().split('-')[0]

    # Check for Plus Code
    if is_plus_code(sheet_postal) or is_plus_code(google_postal):
        return "YES"

    return "YES" if sheet_postal == google_postal else "NO"

def process_poi(row, api_key):
    poi_name = row['poi_nm']
    result = {col: None for col in [
        'is_place_valid', 'is_place_active', 'is_name_correct',
        'is_house_number_correct', 'is_street_name_correct',
        'is_admin2_correct', 'is_admin3_correct', 'is_admin4_correct',
        'is_admin5_correct', 'is_display_location_correct',
        'is_category_name_correct', 'is_postal_code_correct',
        'is_unparsed_addr_correct', 'is_phone_number_correct',
        'google_category', 'google_maps_link', 'official_website',
        'comments'
    ]}
    try:
        lat_long = row['display_lat_long']
        if isinstance(lat_long, str):
            coords = lat_long.strip().replace(' ', ',').split(',')
            if len(coords) != 2:
                raise ValueError(f"Invalid coordinate format: ")
            origin_coords = (float(coords[0]), float(coords[1]))
        else:
            raise ValueError(f"Unsupported coordinate format: ")
        lat, lon = origin_coords
        if not (-90 <= lat <= 90) or not (-180 <= lon <= 180):
            raise ValueError(f"Coordinates out of range: , ")
        nearby_places = get_nearby_places(poi_name, origin_coords[0], origin_coords[1], api_key)
        if nearby_places:
            best_match = None
            best_match_score = 0
            for place in nearby_places:
                place_details = get_place_details(place['place_id'], api_key)
                if place_details:
                    name_match = check_name_similarity(poi_name, place['name'])
                    if name_match == "YES":
                        distance = geodesic(
                            origin_coords,
                            (place['geometry']['location']['lat'],
                             place['geometry']['location']['lng'])
                        ).meters
                        current_score = 70 if name_match == "YES" else 0
                        if current_score > best_match_score:
                            best_match = {
                                'place_details': place_details,
                                'distance': distance,
                                'name': place['name'],
                                'types': place.get('types', [])
                            }
                            best_match_score = current_score
            if best_match:
                details = best_match['place_details']
                house_no = str(row.get('house_no', '')).strip() if row.get('house_no') is not None else ''
                street_number = str(details.get('street_number', '')).strip() if details.get('street_number') is not None else ''
                street_name = str(row.get('streetnm', '')).strip() if row.get('streetnm') is not None else ''
                google_street = str(details.get('street_name', '')).strip() if details.get('street_name') is not None else ''
                admin2 = str(row.get('admin2', '')).strip() if row.get('admin2') is not None else ''
                state = str(details.get('state', '')).strip() if details.get('state') is not None else ''
                admin3 = str(row.get('admin3', '')).strip() if row.get('admin3') is not None else ''
                google_admin3 = str(details.get('admin3', '')).strip() if details.get('admin3') is not None else ''
                admin4 = str(row.get('admin4', '')).strip() if row.get('admin4') is not None else ''
                city = str(details.get('city', '')).strip() if details.get('city') is not None else ''
                admin5 = str(row.get('admin5', '')).strip() if row.get('admin5') is not None else ''
                google_admin5 = str(details.get('admin5', '')).strip() if details.get('admin5') is not None else ''

                result['is_place_valid'] = "YES"
                result['is_place_active'] = is_place_active(details)
                result['is_name_correct'] = "YES"
                result['is_admin2_correct'] = validate_admin2(admin2, state)
                result['is_admin3_correct'] = validate_admin3(admin3, google_admin3)
                result['is_admin4_correct'] = validate_admin4(admin4, city)  # Updated this line
                result['is_admin5_correct'] = validate_admin5(admin5, google_admin5)  # Updated this line
                result['is_postal_code_correct'] = validate_postal_code(row.get('postal_code', ''), details.get('postal_code', ''))
                result['is_street_name_correct'] = validate_street_name(street_name, google_street)
                result['is_house_number_correct'] = "YES" if house_no == street_number else "NO"
                result['is_display_location_correct'] = check_location_accuracy(best_match['distance'])
                result['is_unparsed_addr_correct'] = compare_addresses(row.get('unparsed_addr', ''), details.get('formatted_address', ''))
                result['is_phone_number_correct'] = compare_phone_numbers(row.get('phone', ''), details.get('phone_number', ''))
                result['is_category_name_correct'] = check_category_match(row.get('cat_nm', ''), best_match['types'])
                result['google_category'] = ', '.join(best_match['types'])
                result['google_maps_link'] = details.get('maps_url', '')
                result['official_website'] = details.get('website', '')

                location_comment = (
                    f"Google POI Name: {best_match['name']}\n"
                    f"Distance: {round(best_match['distance'], 2)}m - {check_location_accuracy(best_match['distance'])}"
                )
                if details.get('formatted_address'):
                    location_comment += f"\nGoogle Address: {details['formatted_address']}"
                if details.get('phone_number'):
                    location_comment += f"\nGoogle Phone: {details['phone_number']}"
                result['comments'] = location_comment
                if google_street:
                    location_comment += f"\nGoogle Street: {google_street}"
                if google_admin3:
                    location_comment += f"\nGoogle Admin3: {google_admin3}"
                if google_admin5:
                    location_comment += f"\nGoogle Admin5: {google_admin5}"
            else:
                result['is_place_valid'] = "NO"
                for key in result:
                    if key not in ['google_category', 'google_maps_link', 'official_website', 'comments']:
                        result[key] = "NO"
                result['comments'] = "No matching place found"
        else:
            result['is_place_valid'] = "NO"
            for key in result:
                if key not in ['google_category', 'google_maps_link', 'official_website', 'comments']:
                    result[key] = "NO"
            result['comments'] = "No nearby places found"
    except Exception as e:
        print(f"Error processing : {str(e)}")
        result['is_place_valid'] = "ERROR"
        for key in result:
            if key not in ['google_category', 'google_maps_link', 'official_website', 'comments']:
                result[key] = "ERROR"
        result['comments'] = f"Error: {str(e)}"
    return row.name, result

def generate_search_query(row):
    """Generates a search query string from a row of data."""
    query_fields = [
        str(row.get('poi_nm', '')),
        str(row.get('unparsed_addr', '')),
        str(row.get('admin5', '')),
        str(row.get('admin3', '')),
        str(row.get('admin2', '')),
        str(row.get('admin4', '')),
        str(row.get('postal_code', ''))
    ]
    query = ' '.join(filter(bool, query_fields))
    return query

class AddressVerifier:
    def __init__(self, api_key, search_engine_id):
        self.API_KEY = api_key
        self.SEARCH_ENGINE_ID = search_engine_id
        self.review_keywords = [
            'review', 'reviews', 'rating', 'ratings',
            'customer feedback', 'testimonial', 'testimonials'
        ]
        self.months = [
            'january', 'february', 'march', 'april', 'may', 'june',
            'july', 'august', 'september', 'october', 'november', 'december'
        ]
        self.semaphore = asyncio.Semaphore(10)
        self.browser_timeout = 90000
        self.request_semaphore = asyncio.Semaphore(5)

    @backoff.on_exception(
        backoff.expo,
        (requests.exceptions.RequestException, asyncio.TimeoutError),
        max_tries=3,
        giveup=lambda e: hasattr(e, 'response') and e.response is not None and e.response.status_code < 500
    )
    async def search_google(self, query):
        """Performs a Google Custom Search with exponential backoff."""
        url = "https://www.googleapis.com/customsearch/v1"
        params = {
            'q': query,
            'cx': self.SEARCH_ENGINE_ID,
            'key': self.API_KEY,
            'num': 2
        }
        try:
            async with self.request_semaphore:
                response = requests.get(url, params=params, timeout=30)
                response.raise_for_status()
                data = response.json()
                return list(set(item.get('link', '') for item in data.get('items', [])))
        except requests.exceptions.RequestException as e:
            logger.error(f"Search request failed for query '': ")
            return []

    async def verify_website_address(self, row, url):
        if re.search(r'(download|file=|accesstype=|\.(pdf|csv|txt|xls))', url, re.IGNORECASE):
            logger.warning(f"Skipping problematic URL: ")
            return False, [], False, url

        try:
            if not url.startswith(('http://', 'https://')):
                logger.warning(f"Invalid URL format: ")
                return False, [], False, url

            async with async_playwright() as p:
                browser = await p.chromium.launch(headless=True)
                context = await browser.new_context(
                    viewport={'width': 1280, 'height': 800},
                    user_agent='Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
                )
                page = await context.new_page()

                try:
                    response = await page.goto(url, timeout=self.browser_timeout, wait_until='networkidle')
                    await page.wait_for_timeout(3000)

                    if response and response.status < 400:
                        page_content = await page.content()
                        page_text = page_content.lower()

                        verify_groups = [
                            str(row.get('poi_nm', '')),
                            str(row.get('unparsed_addr', '')),
                            str(row.get('admin5', '')),
                            str(row.get('admin3', '')),
                            str(row.get('admin2', '')),
                            str(row.get('admin4', '')),
                            str(row.get('postal_code', ''))
                        ]

                        group_matches = []
                        for group in verify_groups:
                            if group:
                                is_matched = group.lower() in page_text
                                group_matches.append({
                                    'group': group,
                                    'matched': is_matched
                                })

                        review_exists = False
                        for keyword in self.review_keywords:
                            if keyword in page_text:
                                for month in self.months:
                                    if month in page_text:
                                        review_exists = True
                                        break
                                if review_exists:
                                    break

                        total_matches = sum(1 for match in group_matches if match['matched'])
                        overall_match = total_matches >= 3
                        filtered_matches = [match['group'] for match in group_matches if match['matched']]

                        return overall_match, filtered_matches, review_exists, url
                    else:
                        logger.warning(f"Failed to load page: , Status: {response.status if response else 'No response'}")
                        return False, [], False, url

                except Exception as nav_error:
                    logger.warning(f"Navigation error for : ")
                    return False, [], False, url
                finally:
                    await browser.close()

        except Exception as e:
            logger.error(f"Verification Error for : ")
            return False, [], False, url

    async def process_location(self, row):
        """Processes a single location with consolidated results."""
        search_query = generate_search_query(row)
        urls = await self.search_google(search_query)

        matching_websites = []
        has_reviews = False
        error_urls = []

        # Prioritize TripAdvisor
        tripadvisor_url = next((url for url in urls if "tripadvisor.com" in url), None)
        if tripadvisor_url:
            match_result, match_details, review_exists, url = await self.verify_website_address(row, tripadvisor_url)
            if match_result:
                matching_websites.append(tripadvisor_url)
                has_reviews = has_reviews or review_exists
            else:
                 if url:
                     error_urls.append(url)
            if tripadvisor_url in urls:
                urls.remove(tripadvisor_url)

        for url in urls:
            match_result, match_details, review_exists, current_url = await self.verify_website_address(row, url)
            if match_result:
                matching_websites.append(url)
                has_reviews = has_reviews or review_exists
            elif current_url:
                error_urls.append(current_url)
        status = "ACTIVE" if matching_websites else "UNSURE"


        return {
            '3rd party websites': '; '.join(matching_websites) if matching_websites else 'No matches found',
            'recent_reviews_exist': has_reviews,
            'error_urls' : '; '.join(error_urls) if error_urls else 'No errors found' ,
            'status': status
        }

class IntegratedPOIValidator:
    def __init__(self, maps_api_key, search_api_key, search_engine_id):
        self.maps_api_key = maps_api_key
        self.website_checker = WebsiteChecker()
        self.address_verifier = AddressVerifier(search_api_key, search_engine_id)

    async def process_single_poi(self, row):
        # First run Google Places validation
        idx, places_result = process_poi(row, self.maps_api_key)

        # Get the official website from Places API result
        official_website = places_result.get('official_website', '')

        # If no official website in Places result, use the one from input
        if not official_website and 'official_link' in row:
            official_website = row['official_link']

        # Initialize website status results
        website_status = {
            'website_status': 'NO_WEBSITE',
            'website_check_reason': 'No website available',
            'found_terms': []
        }

        # Only check website if we have a URL
        if official_website:
            try:
                # Create a mock DataFrame row with required fields for website checker
                website_check_data = {
                    'official_link': official_website,
                    'unparsed_addr': row.get('unparsed_addr', ''),
                    'admin1': row.get('admin1', ''),
                    'admin2': row.get('admin2', ''),
                    'admin3': row.get('admin3', ''),
                    'admin4': row.get('admin4', ''),
                    'admin5': row.get('admin5', ''),
                    'postal_code': row.get('postal_code', ''),
                    'street': row.get('streetnm', ''),
                    'house_number': row.get('house_no', ''),
                    'city': row.get('admin4', ''),
                    'state': row.get('admin2', ''),
                    'country': row.get('admin1', '')
                }

                website_result = await self.website_checker.check_website(
                    official_website,
                    website_check_data
                )

                website_status = {
                    'website_status': ('ACTIVE' if website_result['is_active'] else
                                     'CLOSED' if website_result.get('closure_status') else
                                     'INACTIVE'),
                    'website_check_reason': self.website_checker.get_status_reason(website_result),
                    'found_terms': website_result['found_terms']
                }
            except Exception as e:
                website_status = {
                    'website_status': 'ERROR',
                    'website_check_reason': f"Error checking website: {str(e)}",
                    'found_terms': []
                }
        # Process location using AddressVerifier (Code 3)
        third_party_result = await self.address_verifier.process_location(row)

         # Combine results
        combined_result = {**places_result, **website_status, **third_party_result}

        # Conditional adjustments based on is_place_valid, website_status and status
        if combined_result.get('is_place_valid') == 'NO':
             if combined_result.get('website_status') == 'ACTIVE' and combined_result.get('status') == 'ACTIVE':
               columns_to_update = [
                   'is_place_valid', 'is_place_active', 'is_name_correct',
                   'is_house_number_correct', 'is_street_name_correct',
                   'is_admin2_correct', 'is_admin3_correct', 'is_admin4_correct',
                   'is_admin5_correct', 'is_category_name_correct',
                   'is_postal_code_correct', 'is_unparsed_addr_correct',
                   'is_phone_number_correct'
                   ]
               for col in columns_to_update:
                   combined_result[col] = 'YES'
               combined_result['is_display_location_correct'] = 'INCORRECT'

             elif combined_result.get('website_status') == 'INACTIVE' and combined_result.get('status') == 'ACTIVE':
               columns_to_update = [
                   'is_place_valid', 'is_place_active', 'is_name_correct',
                   'is_house_number_correct', 'is_street_name_correct',
                   'is_admin2_correct', 'is_admin3_correct', 'is_admin4_correct',
                   'is_admin5_correct', 'is_category_name_correct',
                   'is_postal_code_correct', 'is_unparsed_addr_correct',
                   'is_phone_number_correct'
                   ]
               for col in columns_to_update:
                   combined_result[col] = 'YES'
               combined_result['is_display_location_correct'] = 'INCORRECT'

             elif combined_result.get('website_status') == 'INACTIVE' and combined_result.get('status') == 'UNSURE':
                columns_to_update = [
                    'is_place_valid', 'is_name_correct',
                    'is_house_number_correct', 'is_street_name_correct',
                    'is_admin2_correct', 'is_admin3_correct', 'is_admin4_correct',
                    'is_admin5_correct', 'is_category_name_correct',
                    'is_postal_code_correct', 'is_unparsed_addr_correct',
                    'is_phone_number_correct'
                ]
                for col in columns_to_update:
                   combined_result[col] = 'YES'
                combined_result['is_place_active'] = 'UNSURE'
                combined_result['is_display_location_correct'] = 'INCORRECT'
        return idx, combined_result
        pass
    async def validate_locations(self):
        print("Please upload your Excel file...")
        uploaded = files.upload()

        for file_name in uploaded.keys():
            file_path = file_name

        df = pd.read_excel(file_path)

        # Add is_place_valid and is_place_active at the beginning
        initial_columns = ['is_place_valid', 'is_place_active']
        for col in reversed(initial_columns):  # Reverse order to maintain correct sequence
            if col not in df.columns:
                df.insert(0, col, "")

        # Define validation columns with their corresponding source columns
        validation_columns = {
            'poi_nm': ['is_name_correct'],
            'house_no': ['is_house_number_correct'],
            'street_nm': ['is_street_name_correct'],
            'admin2': ['is_admin2_correct'],
            'admin3': ['is_admin3_correct'],
            'admin4': ['is_admin4_correct'],
            'admin5': ['is_admin5_correct'],
            'display_lat_long': ['is_display_location_correct'],
            'cat_nm': ['is_category_name_correct'],
            'postal_code': ['is_postal_code_correct'],
            'unparsed_addr': ['is_unparsed_addr_correct'],
            'phone': ['is_phone_number_correct']
        }

        # Add validation columns after their corresponding source columns
        for source_col, validation_cols in validation_columns.items():
            if source_col in df.columns:
                pos = df.columns.get_loc(source_col)
                for i, val_col in enumerate(validation_cols, 1):
                    if val_col not in df.columns:
                        df.insert(pos + i, val_col, "")

        # Additional columns to be added at the end
        additional_columns = [
            'google_category',
            'google_maps_link',
            'official_website',
            'comments',
            'website_status',
            'website_check_reason',
            '3rd party websites',
            'recent_reviews_exist',
            'error_urls',
            'status'
        ]

        # Add remaining columns at the end
        for col in additional_columns:
            if col not in df.columns:
                df[col] = ""

        # Process POIs
        results = []
        for idx, row in df.iterrows():
            print(f"Processing POI {idx + 1}/{len(df)}: {row['poi_nm']}")
            result = await self.process_single_poi(row)
            results.append(result)
            # Add a small delay to avoid rate limits
            await asyncio.sleep(0.2)

        # Update DataFrame with results
        for idx, result in results:
            for key, value in result.items():
                if key in df.columns:
                    df.at[idx, key] = value

        # Save results
        output_file = "integrated_poi_validation_results.xlsx"
        df.to_excel(output_file, index=False)
        files.download(output_file)

        return df

async def main():
    MAPS_API_KEY = 'AlzaSy7-cGfbQHnqkxd1H8kp5-Um2isc0tpdlem'  # Replace with your actual Google Maps API key
    SEARCH_API_KEY = "AIzaSyBy4u6lrDlBc9zespQQQ1Uy-GnzA8gLySY" # Replace with your actual Google Custom Search API key
    SEARCH_ENGINE_ID = "91e8481e423e8475d"

    validator = IntegratedPOIValidator(MAPS_API_KEY, SEARCH_API_KEY, SEARCH_ENGINE_ID)
    await validator.validate_locations()

if __name__ == "__main__":
    asyncio.run(main())
